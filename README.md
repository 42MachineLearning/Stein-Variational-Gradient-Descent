# Variational Gradient Descent (VGD)
Implementation of the method Variational Gradient Descent for scalable Bayesian logistic regression and Bayesian neural network.

There are folders "matlab" and "python" containing implementations of the code in "matlab" and in "python".
  --Matlab code contains an example of Bayesian Logistic Regression 
  --Python code is used to reproduce our table for the example of Bayesian neural network.

## Introduction
VGD is a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. VGD iteratively transports a set of particles to match with the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence.

## Getting Started
  --Our python code is based on Theano 0.8.2

## Basic Usage

## Citation

## Feedback
Feedback is greatly appreciated. If you have any questions, comments, issues or anything else really, [shoot me an email](mailto:dilin.wang.gr@dartmouth.edu).

% Copyright (c) 2016,  Qiang Liu & Dilin Wang
% All rights reserved.
